{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every time after we make a model, we have to check if it is functioning properly or not. But testing on the data that was used to build the model is a bad idea. When testing if one performs well, it should be tested on a separate data set which hasn't been used for training which is the reason before doing anything, we separate the whole data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we build a model, we test it on testing set over and over again until it becomes accurate enough to be used. <br>\n",
    "A problem in this case is this is also a bad practice. Doing this will cause the model to be overfitting on testing data that when some new data is inserted, it may work poorly. This is a reason that testing set should only be used once throughout the entire training and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we cannot use the testing set, how can we check if a model is working well? We cannot use training nor testing set. For this reason we have something called Cross Validation. This is a simple idea. We just divide a training set into $k$ sets and train a model with $k-1$ set and get an error value with the remaining set. After we divided and used the last set to train, next thing we do is initialize a new model and train it with different combinations of $k-1$ sets and test with the one left. We do this process until there is no more combination of sets we haven't used to train model. Now we have $k$ different error values. We get the mean value of it and that will become our final performace score (or error value) of the model. Also by doing cross validation, we can check which subset of features produce high or low error and select the ones with low value.<br>\n",
    "The following is a pseudo-code of cross validation."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-12T23:21:00.779577Z",
     "start_time": "2018-10-12T23:21:00.768370Z"
    }
   },
   "source": [
    "function(features, x, y, k)\n",
    "\n",
    "error = []\n",
    "\n",
    "  for k times\n",
    "    size_of_each_set = ceiling(length(x) / k)\n",
    "    \n",
    "    validation = x[length(x) - size :]\n",
    "    v_label = y[length(x) - size : ]\n",
    "    \n",
    "    training = x[:length(x) - size]\n",
    "    t_label = y[:length(x) - size]\n",
    "    \n",
    "    model = init\n",
    "    model.train(training, t_label)\n",
    "    \n",
    "    pred = model.predict(validation)\n",
    "    error = append(loss(v_label), pred)\n",
    "\n",
    "return mean of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above code, the x is the whole training set (not the whole data) and y is the label corresponding to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily do the cross validation with existing library (scikit). But since the idea is simple and implementation is also easy do, let's try implementing it on our own first, and then see how to use the library. <br>\n",
    "Again, we will use linear regression with housing price data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T00:27:19.433788Z",
     "start_time": "2018-10-21T00:27:18.865855Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T23:39:14.663046Z",
     "start_time": "2018-10-20T23:39:14.520022Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T23:39:14.686019Z",
     "start_time": "2018-10-20T23:39:14.668021Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.loc[:, list(train)[:-1]]\n",
    "y = train.loc[:, 'SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-20T23:50:46.964401Z",
     "start_time": "2018-10-20T23:50:46.921401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsong1101\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# set split size to 0.8\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T00:24:40.440217Z",
     "start_time": "2018-10-21T00:24:40.428229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Root Mean Squared Error\n",
    "def rmse(y, y_hat):\n",
    "    return np.sqrt(np.mean((y - y_hat)**2))\n",
    "\n",
    "def get_index(x, size):\n",
    "    index = []\n",
    "    validation = []\n",
    "    r = set([i for i in range(len(x))])\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        validation.append(i)\n",
    "\n",
    "        if (i+1) % size == 0 or i == len(x)-1:\n",
    "            \n",
    "            training = list(r - set(validation))\n",
    "            index.append([np.array(training), np.array(validation)])\n",
    "            validation = []\n",
    "\n",
    "    return index\n",
    "\n",
    "def cross_validation(x, y, k):\n",
    "    \n",
    "    size = ceil(len(x) / k)\n",
    "    \n",
    "    # list of list of indices to split\n",
    "    index = get_index(x, size)\n",
    "    error = []\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    for training, validation in index:\n",
    "#         print(training)\n",
    "        train = x.iloc[training]\n",
    "        test = y.iloc[training]\n",
    "        \n",
    "#         print(type(training))\n",
    "#         print(validation)\n",
    "        break\n",
    "#         model.fit(train, test)\n",
    "        \n",
    "#         validate = x[!ind]\n",
    "#         t_validate = y[!ind]\n",
    "    \n",
    "#         error.append(rmse(model.predict(validate), t_validate))\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T00:25:58.783643Z",
     "start_time": "2018-10-21T00:25:58.775646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 80)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T00:26:22.900600Z",
     "start_time": "2018-10-21T00:26:22.887653Z"
    }
   },
   "outputs": [],
   "source": [
    "ind = cross_validation(X_train, X_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
