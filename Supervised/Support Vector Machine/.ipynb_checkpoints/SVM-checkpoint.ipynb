{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post, we talked about how a decision tree (link here) can classify different targets in Iris dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to explore another supervised machine learning called 'Support Vector Machine' or SVM which is also useful when classifying objects and will use the same Iris data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![svm](wiki_svm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, general idea of SVM is to put a boundary (or multiple boundaries for multi-classification) between distinct classes while maximizing the margin between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we draw a line between each object, it is either linearly separable as the right figure above or non-linearly separable like the left figure. In the second case we have to transform data samples in some ways so that it is converted to be linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tranformation can be done through Kernel Trick which will be covered later in the post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example1](example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we have two classes $1$, black circles and $-1$, hollow circles. Our decision boundary in the middle ($wx - b = 0$) is the line that we have to figure out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above figure, there are two other lines $wx - b = 1$ and $wx - b = -1$ and these are the lines that contain points closest to our decision boundary and these points are called 'Support Vector'. Based on these vectors we are deciding the boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other non-support vectors don't affect the location of the boundary. Consider the following situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example1](example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the decision boundary is exactly in the middle of support vectors of two classes, it doesn't matter if other vectors move to other points or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in figure 2, the distance between any support vector and the line of the boundary is $\\frac{1}{\\lVert w \\rVert}$ and it is easy to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $wx + b = 0$ be the boundary line and $x_0, y_0$ be any point of a support vector. The distance between a point and a line is as given. $$d = \\frac{|Ax_0 + By_0 + C|}{\\sqrt{A^2 + B^2}}$$\n",
    "$x_0$ and $y_0$ are the coordinates of a point of a support vector and $A$ and $B$ are the coordinates of $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ax_0 + By_0$ is $wx$ and $C$ is the bias that it can be rewritten as $wx + b = 1$ for the line passing through a support vector of class 1. Hence the numerator becomes 1 for both $1$ and $-1$ as it is the absolute value. So the distance between the boundary to a support vector is $\\frac{1}{\\lVert w \\rVert}$ and the distance between support vectors of two classes is $\\frac{2}{\\lVert w \\rVert}$ just like in the figure 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM maximizes the margin between classes and to do so, we have to find the smallest possible $w$ value that as $w \\rightarrow 0$, then $d \\rightarrow \\infty$. Finding $w$ is quadratic problem which will be covered in a different post regarding optimization. This quadratic problem has the following form.\n",
    "$$\\min_{w} \\frac{1}{2} \\lVert w \\rVert^2$$ subject to $$y(wx + b) \\geq 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding this $w$ completes constructing a SVM model for linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider a situation where our data cannot be split nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example3](example3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left figure, blue and green circles cannot be divided with a straight line so we have to somehow transform this data to look like the right figure. This is one of main advantages of SVM. What it does is to transform data into a higher dimension so that we can split data with a straight line. In above example, points in the left are in one-dimension while the points in the right are in two-dimension. The transformation is $T(x): x \\rightarrow x^2$. Note that we are not actually changing the whole data but just finding a right function $T(x)$ we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we transform data the rest procedure is to find $w$ again for a line or a plane that separates data, same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing is very simple with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T05:30:36.443333Z",
     "start_time": "2019-01-15T05:30:35.255226Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T05:32:17.826844Z",
     "start_time": "2019-01-15T05:32:17.815877Z"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iris1](iris1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are three classes and it is not linearly separable. Let's see how a SVM model can classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T05:36:45.046099Z",
     "start_time": "2019-01-15T05:36:45.012096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsong1101\\anaconda3\\envs\\p\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
