{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will go through what Linear Regression is and how to implement it from the scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linear Regression GIF](anim1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is Linear Regression? We use this when we want to find out an approximate linear relation between a feature and the remaining features of a data set. This relation can be proportional or disproportional or none at all. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Different Relations](relation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a linear regression model, we can predict (approximate) values. Examples where we can use this model are Height vs Weight, Income vs Age, Price of Houses vs Quality, and many more. In most cases, it is impossible to predict with no errors and the model tries to find a relation with the least errors. (This error is also called <u>residuals</u>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several versions of linear regression such as ordinary least squares (OLS), generalized least sqaures (GLS), weighted least squares (WLS) and so on, with or without other forms of regularization but in this post, OLS will be covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T16:41:10.299134Z",
     "start_time": "2018-08-20T16:41:10.293841Z"
    }
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:02.695739Z",
     "start_time": "2018-12-24T02:45:02.691738Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding further, let's use weight vs height data set from Kaggle for this exercise. You can find them [here](https://www.kaggle.com/mustafaali96/weight-height)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you download the data and put it in the same folder you are coding, let's load them and check the first five rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:03.240506Z",
     "start_time": "2018-12-24T02:45:03.200507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Male</td>\n",
       "      <td>73.847017</td>\n",
       "      <td>241.893563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>68.781904</td>\n",
       "      <td>162.310473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>74.110105</td>\n",
       "      <td>212.740856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>71.730978</td>\n",
       "      <td>220.042470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Male</td>\n",
       "      <td>69.881796</td>\n",
       "      <td>206.349801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Gender     Height      Weight\n",
       "0   Male  73.847017  241.893563\n",
       "1   Male  68.781904  162.310473\n",
       "2   Male  74.110105  212.740856\n",
       "3   Male  71.730978  220.042470\n",
       "4   Male  69.881796  206.349801"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('./weight-height.csv')\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:03.333515Z",
     "start_time": "2018-12-24T02:45:03.324510Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10000 samples and have $Gender, Height, Weight$ columns. Let's drop the Gender column so that we only have height and weight vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can still use the $Gender$ column and using two separate models and compare the relation of weight and height by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:03.713507Z",
     "start_time": "2018-12-24T02:45:03.705509Z"
    }
   },
   "outputs": [],
   "source": [
    "dat = dat.drop('Gender', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-20T05:03:05.914715Z",
     "start_time": "2018-12-20T05:03:05.906715Z"
    }
   },
   "source": [
    "There are two versions when constructing a linear regression model.\n",
    "1. One whose line goes through the origin.\n",
    "2. One whose line doesn't go through the origin (model with bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a model that goes through the origin and compare it later to the one that doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the first model, we can build a model in two ways. One is using a closed-form equation and one is using a loop to find an appropriate weight $w$ (or slope) of the line with the least errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for the closed form is as follows.\n",
    "$$w = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we advance, let's first graph them out to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scatter of Height vs Weight](scatter1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily assume that the taller people get, the heavier the become."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the equation above, we can get the $w$ by the following. (The @ means matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:05.799317Z",
     "start_time": "2018-12-24T02:45:05.791324Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4502"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dat['Height']\n",
    "y = dat['Weight']\n",
    "\n",
    "closed_w = 1 / (X.T @ X) * X.T @ y\n",
    "\n",
    "round(closed_w, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the points with our simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scatter of Height vs Weight](scatter2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the green line goes through the center of the data points, when there is obviously a better model with lower error values. This is due to the absence of the bias term. Let's look at the same graph as above but with wider x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:22:29.117103Z",
     "start_time": "2018-12-24T02:22:29.085101Z"
    }
   },
   "source": [
    "![Scatter of Height vs Weight](scatter3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the line has to go through the origin, the best solution it can generate is to go through the center of the data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of using the closed-form equation, let's construct one using gradient descent. To do this, we need two helper functions and one main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:09.318042Z",
     "start_time": "2018-12-24T02:45:09.311045Z"
    }
   },
   "outputs": [],
   "source": [
    "def grad_mse_weight(w, x, y, b):\n",
    "    \n",
    "    return -2 * np.mean((y - (w*x + b)) * x)\n",
    "\n",
    "def grad_mse_bias(w, x, y, b):\n",
    "    \n",
    "    return -2 * np.mean(y - (w*x + b))\n",
    "\n",
    "def predict(x, y, learning_rate=0.1):\n",
    "    \n",
    "    w = 0\n",
    "    b = 0\n",
    "    \n",
    "    iter_num = 10000\n",
    "    \n",
    "    for i in range(iter_num):\n",
    "        \n",
    "        dw = grad_mse_weight(w, x, y, b)\n",
    "        db = grad_mse_bias(w, x, y, b)\n",
    "        \n",
    "        b -= db * learning_rate\n",
    "        w -= dw * learning_rate\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two functions are used to determine the ideal slope of the linear regression (or the weight) and the bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not familiar with gradient descent, you can find them [here!](https://tlr.gitbook.io/data-science/machine-learning-basics/gradient-descent). To explain it briefly, these two functions compute a better value for bias and w at each iteration and update them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <code>predict(x,y)</code> function, we are looping 10000 times and updating weight and bias every iteration. You can try putting different values for <code>learning_rate</code> and <code>iter_num</code> to acquire a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train and test our model, it is a good practice to divide our data into two portions. One for training and one for testing the model. This is to avoid over-fitting of the model. <strike>You can find a post about over-fitting and under-fitting of a model [here](). To be updated</strike>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:11.418190Z",
     "start_time": "2018-12-24T02:45:11.411195Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing you should know is that our height and weight values are quite big that the loss function we are using (Mean Squared Errors) leads to exploding gradient descent values that the resulting bias and weight after predict function will likely generate NaN values. We can verify this with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:45:23.119048Z",
     "start_time": "2018-12-24T02:45:12.800048Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hsong1101\\anaconda3\\envs\\p\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan, nan)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = predict(X_train, y_train)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the values for $w$ and $b$ are both nan values. This is because of too much update on both at each iteration. The following statement is the first five $w$ and $b$ values at each iteration (This could be different with different hyperparameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 0.3228807136656615 21.65726752224676\n",
    "2. -2.2295643269137306 -148.1549601788397\n",
    "3. 17.763141860690368 1183.3266020093588\n",
    "4. -139.01850164601672 -9256.691211902544\n",
    "5. 1090.2704303466826 72602.47884447992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of reasons we normalize our data before setting up a model. By doing this, we are scaling all numeric values to range from 0 to 1 and doing this will prevent the model from generating any NaN values or exploding gradient descent as well as faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another reason why we normalize. Our heights and weights have different scale that heights range from around 50 to 80 while weights range from 50 to 280. What if we changed the scale of heights from inches to feet or any other metric. Then we cannot use the same $w$ and $b$ as that model would work poorly due to different relation with different scale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we both scale them to range from 0 to 1, then it doesn't matter if the heights value are in inches, feet or centimeters as the relation between two features will always have the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's normalize our data and find an appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:37.973131Z",
     "start_time": "2018-12-24T02:27:37.935130Z"
    }
   },
   "outputs": [],
   "source": [
    "x1 = X_train / X_train.max()\n",
    "y1 = y_train / y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.098133Z",
     "start_time": "2018-12-24T02:27:37.976131Z"
    }
   },
   "outputs": [],
   "source": [
    "w, b = predict(x1, y1)\n",
    "\n",
    "round(w, 4), round(b, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our data with trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scatter of Height vs Weight](scatter4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what we need to do is to predict and compare with true y values. Note that we have to scale them using the same <code>X_train.max</code> and <code>y_train.max</code> values for consistency or else the scale of the result will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.108136Z",
     "start_time": "2018-12-24T02:27:48.101176Z"
    }
   },
   "outputs": [],
   "source": [
    "x2 = X_val / X.max()\n",
    "y2 = y_val / y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is our testing set with prediction line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scatter of Height vs Weight](scatter5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, let's see how much the error is. For the prediction, we should scale them back to its original form to compare with true y values. To do that, we can just multiply the y_max value to the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.129134Z",
     "start_time": "2018-12-24T02:27:48.111136Z"
    }
   },
   "outputs": [],
   "source": [
    "def MSE(pred, y):\n",
    "    return np.mean(np.square(y - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.157151Z",
     "start_time": "2018-12-24T02:27:48.132132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Using Closed-Form Equation : 1019.9261523310762\n",
      "Error Using Gradient Descent : 153.93702071075964\n"
     ]
    }
   ],
   "source": [
    "print('Error Using Closed-Form Equation : {}'.format(MSE(X_val*w, y_val)))\n",
    "print('Error Using Gradient Descent : {}'.format(MSE((x2*w+b) * y.max(), y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use linear regression even on functions that are not linear as long as our weight vectors are linear. For example, consider the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.184127Z",
     "start_time": "2018-12-24T02:27:48.161130Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "y = x + np.sin(x)\n",
    "noise = np.random.normal(0, 0.7, 100)\n",
    "true_y = y + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sin Model](sin_model_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated with $f(x) = x + sin(x)$ with some noise. If we make a model with the function defined above, it will look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.489127Z",
     "start_time": "2018-12-24T02:27:48.187129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0211549273118783, 1.1088907569956085e-16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b = predict(x, y, learning_rate=0.01)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sin Model](sin_model_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.504130Z",
     "start_time": "2018-12-24T02:27:48.497132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Value : 0.7936810997236736\n"
     ]
    }
   ],
   "source": [
    "print('Error Value : {}'.format(MSE(w*x + b, true_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our model with the function $f(x) = \\theta_1 x + sin(\\theta_2 x)$. The next functions are gradient descent to compute $\\theta_1$ and $\\theta_2$. Since the above data goes through the origin, we don't have to include a bias term this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.529127Z",
     "start_time": "2018-12-24T02:27:48.508132Z"
    }
   },
   "outputs": [],
   "source": [
    "def model(x, theta):\n",
    "    return x*theta[0] + np.sin(x*theta[1])\n",
    "\n",
    "def grad_dt1(x, y, theta):\n",
    "    \n",
    "    return -2 * np.mean(x * (y - theta[0] * x - np.sin(theta[1] * x)))\n",
    "\n",
    "def grad_dt2(x, y, theta):\n",
    "    \n",
    "    return -2 * np.mean((y - (theta[0] * x + np.sin(theta[1] * x))) * (x * np.cos(theta[1] * x)))\n",
    "\n",
    "def predict(x, y, theta, learning_rate=0.01):\n",
    "    \n",
    "    t1, t2 = 0, 0\n",
    "    \n",
    "    iter_num = 20\n",
    "    \n",
    "    for i in range(iter_num):\n",
    "        \n",
    "        dt1 = grad_dt1(x, y, theta)\n",
    "        dt2 = grad_dt2(x, y, theta)\n",
    "        \n",
    "        t1 -= dt1 * learning_rate\n",
    "        t2 -= dt2 * learning_rate\n",
    "        \n",
    "        \n",
    "    return t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.564137Z",
     "start_time": "2018-12-24T02:27:48.532134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9723320317973033, 0.9723320317973033)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1, t2 = predict(x, y, [0, 0], learning_rate=0.0007)\n",
    "t1, t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:23:55.086344Z",
     "start_time": "2018-12-24T02:23:55.054390Z"
    }
   },
   "source": [
    "![Sin Model](sin_model_complex.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T02:27:48.583130Z",
     "start_time": "2018-12-24T02:27:48.568137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Value : 0.5169340003737359\n"
     ]
    }
   ],
   "source": [
    "print('Error Value : {}'.format(MSE(model(x, [t1, t2]), y+noise)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've tried many different learning_rate value to have one that fits well to the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post only deals with the basic linear regression without any regularization such as Lasso, Ridge or Elastic Net. There are many versions of it besides Ordinary Least Squares. These topics will be covered in later posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you all for reading and if you find any errors or typos or have any suggestions, please let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you all for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
